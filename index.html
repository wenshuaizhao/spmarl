<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="We show two flaws in existing reward based curriculum learning algorithms when generating number of agents as curriculum in MARL. Instead, we propose a learning progress metric as a new optimization objective which generates curriculum maximizing the learning progress of agents.">
  <meta name="keywords" content="Multi-Agent Learning, Curriculum Learning, Learning Progress">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Learning Progress Driven Multi-Agent Curriculum</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/robot-svgrepo-com.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://wenshuaizhao.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          
          <a class="navbar-item" href="https://arxiv.org/pdf/2311.01953v2">
            OptimisticMAPG
          </a>
          <a class="navbar-item" href="https://arxiv.org/pdf/2401.08728v3">
            AgentMixer
          </a>
          <a class="navbar-item" href="https://arxiv.org/pdf/2401.12574v1">
            BTA
          </a>
          <a class="navbar-item" href="https://proceedings.mlr.press/v202/zhao23k/zhao23k.pdf">
            TCRL
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Learning Progress Driven Multi-Agent Curriculum</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://wenshuaizhao.github.io/">Wenshuai Zhao</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://lizhyun.github.io/">Zhiyuan Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://rl.aalto.fi/team/">Joni Pajarinen</a><sup>1</sup>
            </span>
            
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Department of Electrical Engineering and Automation, Aalto University</span>
            <!-- <span class="author-block"><sup>2</sup>University of Electronic Science and Technology of China,</span>
            <span class="author-block"><sup>3</sup>University of Oulu</span> -->
            
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2205.10016"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2205.10016"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/wenshuaizhao/spmarl"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>






<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The number of agents can be an effective curriculum variable for controlling the difficulty of multi-agent reinforcement learning (MARL) tasks.
Existing work typically uses manually defined curricula such as linear schemes. We identify two potential flaws while applying existing reward-based automatic curriculum learning methods in MARL: (1) The expected episode return used to measure task difficulty has high variance; (2) Credit assignment difficulty can be exacerbated in tasks where increasing the number of agents yields higher returns which is common in many MARL tasks.
To address these issues, we propose to control the curriculum by using a TD-error based <i>learning progress </i> measure and by letting the curriculum proceed from an initial context distribution to the final task specific one. 
Since our approach maintains a distribution over the number of agents and
measures learning progress rather than absolute performance, which often increases with the number of agents, we alleviate problem (2). Moreover, the learning progress measure naturally alleviates problem (1) by
aggregating returns. In three challenging sparse-reward MARL benchmarks, our approach outperforms state-of-the-art baselines.
          </p>
          
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            In the MPE <i>Simple-Spread</i> task, agents (blue circles) need to cover as many landmarks (red circles) as possible. 
            With the number of landmarks fixed, 20 agents shown on the right can easily complete the task and achieve higher returns compared to 8 agents on the left. 
            However, a higher number of agents exacerbates the credit assignment problem in policy learning.
          </p>
        </div>

        <div class="content" style="display: flex; justify-content: center; align-items: flex-start; gap: 40px;">
          <figure style="text-align: center; margin: 0;">
              <img src="./docs/mpe_8_agents.png" width="200" />
              <figcaption>8 agents in MPE</figcaption>
          </figure>
          <figure style="text-align: center; margin: 0;">
              <img src="./docs/mpe_20_agents.png" width="200" />
              <figcaption>20 agents in MPE</figcaption>
          </figure>
      </div>
      
      <br/>

        <h2 class="title is-3">Experiments</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Simple Spread</h3>
        <div class="content has-text-justified">
          <p>
            Comparison on the <em>Simple-Spread</em> task, where the target is set with <span style="font-style: italic;">8</span> agents and <span style="font-style: italic;">8</span> landmarks. 
            The plots are averaged over 5 random seeds and the shadow area denotes the 95% confidence intervals. 
            The <strong>left</strong> figure shows the evaluation returns on the target task with <span style="font-style: italic;">8</span> agents. 
            Note that the x-axis represents the samples collected from the environment, which is proportional to the number of agents. 
            The <strong>middle</strong> figure presents the generated curriculum from different methods, 
            where SPMARL and SPRLM first generate more agents and then converge to the target <span style="font-style: italic;">8</span> agents, 
            while ALPGMM and VACL always generate more agents. 
            The <strong>right</strong> figure shows the episode returns on the training tasks. 
            The ALPGMM algorithm achieves the highest because it samples tasks with more than <span style="font-style: italic;">14</span> agents.
        </p>
        </div>
        <div class="content has-text-justified">
            <!-- <div class="column is-3 has-text-centered"> -->
              <img src="./docs/simple_spread.png" width="800" class="center"/>
              <!-- <p>Start Frame</p> -->
            <!-- </div> -->
          </div>
        
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">XOR Game</h3>
        <div class="content has-text-justified">
          <p>
            Comparison on the 20-player <em>XOR</em> game where each agent needs to output different actions to succeed. 
            While the linear curriculum from few to more (<em>linear</em>) and <em>alpgmm</em> successfully achieve optima eventually, 
            SPRLM and SPMARL demonstrate a faster convergence.
        </p>
        
        </div>
        <div class="content has-text-justified">
            <!-- <div class="column is-3 has-text-centered"> -->
              <img src="./docs/matrix.png" width="800" class="center"/>
              <!-- <p>Start Frame</p> -->
            <!-- </div> -->
          </div>
        <!--/ Re-rendering. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">SMAC-v2 Tasks</h3>
        <div class="content has-text-justified">
          <p>
            Comparison on SMACv2 <em>Protoss</em> tasks. From top to bottom row, the tasks are 
            <em>Terran 5 vs. 5</em>, <em>Terran 6 vs. 6</em>, <em>Zerg 5 vs. 5</em>, and <em>Zerg 6 vs. 6</em>. 
            Across all four tasks, SPMARL achieves performance that is comparable to or better than all baseline methods.
        </p>
        
        </div>
        <div class="content has-text-justified">
            <!-- <div class="column is-3 has-text-centered"> -->
              <img src="./docs/terran_5v5.png" width="800" class="center"/>
              <img src="./docs/terran_6v6.png" width="800" class="center"/>
              <img src="./docs/zerg_5v5.png" width="800" class="center"/>
              <img src="./docs/zerg_5v5.png" width="800" class="center"/>
              <!-- <p>Start Frame</p> -->
            <!-- </div> -->
          </div>
        <!--/ Re-rendering. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Ablation on V<sub style="font-size: 0.8em;">LB</sub></h3>
        <div class="content has-text-justified">
          <p>
            Ablation of V<sub style="font-size: 0.8em;">LB</sub> on SMACv2 <em>Protoss</em> tasks. 
            From top to bottom, the tasks are <em>Protoss 5 vs. 5</em> and <em>Protoss 6 vs. 6</em>. 
            The results indicate that SPMARL performs robustly across a broad range of V<sub style="font-size: 0.8em;">LB</sub>.
        </p>
        </div>
        <div class="content has-text-justified">
          <!-- <div class="column is-3 has-text-centered"> -->
            <img src="./docs/protoss_5v5_ablate.png" width="800" class="center"/>
            <img src="./docs/protoss_6v6_ablate.png" width="800" class="center"/>
           
          <!-- </div> -->
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->



  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{zhao2025learning,
        title={Learning Progress Driven Multi-Agent Curriculum},
        author={Zhao, Wenshuai and Li, Zhiyuan and Pajarinen, Joni},
        booktitle={Proceedings of the International Conference on Machine Learning},
        year={2025}
      }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <!-- <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p> -->
          <p>
            This website borrows code from this <a
              href="https://github.com/nerfies/nerfies.github.io">template</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>